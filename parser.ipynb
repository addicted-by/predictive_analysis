{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyML9tZpt4q7g23siokENAjo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cs3IIES4CYNI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(json_file: str='/content/help.json') -> dict:\n",
        "  global names, scales\n",
        "  f = open(json_file)\n",
        "  data = json.load(f)\n",
        "  names = data['names']\n",
        "  names_scaled = data['names_scaled']\n",
        "  names_unscaled = data['names_unscaled']\n",
        "  scales = data['scales']\n",
        "  arrays = {}\n",
        "  for name in names:\n",
        "    loaded_arr = np.loadtxt(name)\n",
        "  \n",
        "    arrays[name] = loaded_arr.reshape(\n",
        "        loaded_arr.shape[0], loaded_arr.shape[1] // scales[name][2], scales[name][2])\n",
        "  f.close()\n",
        "  return arrays"
      ],
      "metadata": {
        "id": "-obh1jnvCnUl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = load_data()"
      ],
      "metadata": {
        "id": "8fXrdIezCqCe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(arr['(scaled)graphother_22.txt'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU_eeUG5DIJ9",
        "outputId": "aa342b6d-0794-4de8-af3c-4468907130c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qFxVDUuFh3M",
        "outputId": "42504098-82c6-4e20-9222-da305fa7c7e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(scaled)graphother_22.txt',\n",
              " '(unscaled)graphother_22.txt',\n",
              " '(scaled)graphvalve1_14.txt',\n",
              " '(unscaled)graphvalve1_14.txt',\n",
              " '(scaled)graphvalve2_3.txt',\n",
              " '(unscaled)graphvalve2_3.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure',\n",
        "       'Temperature', 'Thermocouple', 'Voltage', 'Volume Flow RateRMS',\n",
        "       'anomaly', 'changepoint']"
      ],
      "metadata": {
        "id": "F9pQ9D4jDnc-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies = []\n",
        "cps = []\n",
        "anomaly = []"
      ],
      "metadata": {
        "id": "dv8WA01NFnuW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "result_csp = []\n",
        "for j in range(len(names)):\n",
        "  csp = arr[names[j]][0]\n",
        "  for i in range(1, len(arr[names[j]])):\n",
        "    csp = np.concatenate((csp, arr[names[j]][i]), axis=0)\n",
        "  result_csp.append(csp)\n",
        "anomalies = copy.deepcopy(result_csp)\n",
        "csp = copy.deepcopy(result_csp)"
      ],
      "metadata": {
        "id": "QfpeendCG3cI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(anomalies[0][0:,9:10]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lczINHS3jbid",
        "outputId": "84f56733-6343-4288-cbda-1adb92b9270e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25320, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(result_csp)):\n",
        "  anomaly.append(anomalies[i][0:,8:9])\n",
        "  anomaly[i] = np.round_(anomaly[i])\n",
        "  result_csp[i] = result_csp[i][0:,0:8]\n",
        "  cps.append(anomalies[i][0:, 9:])\n",
        "  cps[i] = np.round_(cps[i])\n",
        "  result_csp[i] = result_csp[i][0:,0:8]"
      ],
      "metadata": {
        "id": "Pv6XZ7Nij6MG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy.signal import medfilt\n",
        "from sklearn import decomposition\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from itertools import product"
      ],
      "metadata": {
        "id": "sVSsUw05ke2I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for repeatability\n",
        "def Random(seed_value):\n",
        "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "    import os\n",
        "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "    import random\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "    import numpy as np\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(seed_value)"
      ],
      "metadata": {
        "id": "ibGx8E6lklLr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "x4Oqj_EykpuX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arch(data):\n",
        "    Random(0)\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=(data.shape[1], data.shape[2])),\n",
        "            layers.Conv1D(\n",
        "                filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Dropout(rate=0.2),\n",
        "            layers.Conv1D(\n",
        "                filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Conv1DTranspose(\n",
        "                filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Dropout(rate=0.2),\n",
        "            layers.Conv1DTranspose(\n",
        "                filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
        "        ]\n",
        "    )\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-3), loss=\"mse\")\n",
        "    # model.compile(optimizer=keras.optimizers.experimental.AdamW(learning_rate=0.001), loss=\"mse\")\n",
        "\n",
        "    # model.summary()\n",
        "\n",
        "    history = model.fit(\n",
        "        data,\n",
        "        data,\n",
        "        epochs=100,\n",
        "        batch_size=128,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "        # metrics=[f1]#,\n",
        "        # callbacks=[\n",
        "        #     keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\", verbose=0)\n",
        "        # ],\n",
        "    )\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "RpjXqh6chFWq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generated training sequences for use in the model.\n",
        "def create_sequences(values, time_steps=24):\n",
        "    output = []\n",
        "    for i in range(len(values) - time_steps + 1):\n",
        "        output.append(values[i : (i + time_steps)])\n",
        "    return np.stack(output)"
      ],
      "metadata": {
        "id": "h8iZrUldkus8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2n2l4gVA32ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters selection\n",
        "N_STEPS = 24\n",
        "Q = 0.85 # quantile for upper control limit (UCL) selection\n",
        "from tqdm import tqdm\n",
        "# inference\n",
        "predicted_outlier, predicted_cp, out = [], [], []\n",
        "for df in tqdm([result_csp[0]]):\n",
        "    # X_train = df[:400].drop(['anomaly', 'changepoint'], axis=1)\n",
        "    X_train = df[:6000]\n",
        "    # scaler init and fitting\n",
        "    # StSc = StandardScaler()\n",
        "    # StSc.fit(X_train)\n",
        "    # convert into input/output\n",
        "    # X = create_sequences(StSc.transform(X_train), N_STEPS)\n",
        "    X = create_sequences(X_train, 24)\n",
        "    # model defining and fitting\n",
        "    history, model = arch(X)\n",
        "    out.append(history.history)  \n",
        "    # results predicting\n",
        "    residuals = pd.Series(np.sum(np.mean(np.abs(X - model.predict(X)), axis=1), axis=1))\n",
        "    UCL = residuals.quantile(Q)\n",
        "    \n",
        "    # results predicting\n",
        "    # X = create_sequences(X, N_STEPS)\n",
        "    cnn_residuals = pd.Series(np.sum(np.mean(np.abs(X - model.predict(X)), axis=1), axis=1))\n",
        "    \n",
        "    # data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
        "    anomalous_data = cnn_residuals > (3/2 * UCL)\n",
        "    anomalous_data_indices = []\n",
        "    for data_idx in range(N_STEPS - 1, len(X) - N_STEPS + 1):\n",
        "        if np.all(anomalous_data[data_idx - N_STEPS + 1 : data_idx]):\n",
        "            anomalous_data_indices.append(data_idx)\n",
        "    \n",
        "    prediction = pd.Series(data=0)\n",
        "    prediction.iloc[anomalous_data_indices] = 1\n",
        "    \n",
        "    # predicted outliers saving\n",
        "    predicted_outlier.append(prediction)\n",
        "    \n",
        "    # predicted CPs saving\n",
        "    prediction_cp = abs(prediction.diff())\n",
        "    prediction_cp[0] = prediction[0]\n",
        "    predicted_cp.append(prediction_cp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gDTjsQxk0DQ",
        "outputId": "9d81e9c9-939b-402d-ed11-96d2ffc81ff8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 28ms/step - loss: 0.1933 - val_loss: 0.1278\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1199 - val_loss: 0.1215\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1154 - val_loss: 0.1204\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1143 - val_loss: 0.1194\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1138 - val_loss: 0.1190\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1136 - val_loss: 0.1189\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1135 - val_loss: 0.1189\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1134 - val_loss: 0.1188\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1133 - val_loss: 0.1188\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1133 - val_loss: 0.1189\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1132 - val_loss: 0.1187\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1131 - val_loss: 0.1187\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1131 - val_loss: 0.1187\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1131 - val_loss: 0.1187\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1130 - val_loss: 0.1186\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1130 - val_loss: 0.1187\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1130 - val_loss: 0.1186\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1130 - val_loss: 0.1186\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1130 - val_loss: 0.1185\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1130 - val_loss: 0.1185\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1130 - val_loss: 0.1185\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1129 - val_loss: 0.1186\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1129 - val_loss: 0.1186\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1129 - val_loss: 0.1185\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1186\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1186\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1183\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1128 - val_loss: 0.1183\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.1128 - val_loss: 0.1183\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1183\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1128 - val_loss: 0.1184\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1183\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1127 - val_loss: 0.1184\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1128 - val_loss: 0.1184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 1/6 [00:28<02:24, 28.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 29ms/step - loss: 0.1923 - val_loss: 0.1279\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1196 - val_loss: 0.1206\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1146 - val_loss: 0.1192\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1136 - val_loss: 0.1187\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1133 - val_loss: 0.1186\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1131 - val_loss: 0.1186\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1130 - val_loss: 0.1186\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1129 - val_loss: 0.1184\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1128 - val_loss: 0.1185\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1127 - val_loss: 0.1184\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1127 - val_loss: 0.1183\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1127 - val_loss: 0.1184\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1126 - val_loss: 0.1183\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1126 - val_loss: 0.1183\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1126 - val_loss: 0.1183\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1126 - val_loss: 0.1183\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1126 - val_loss: 0.1182\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1125 - val_loss: 0.1181\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1125 - val_loss: 0.1181\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - val_loss: 0.1182\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1182\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1180\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1124 - val_loss: 0.1180\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.1124 - val_loss: 0.1180\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1181\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1180\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1180\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1124 - val_loss: 0.1180\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1124 - val_loss: 0.1180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:57<01:54, 28.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 29ms/step - loss: 0.1328 - val_loss: 0.0503\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0459 - val_loss: 0.0384\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0417 - val_loss: 0.0377\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0406 - val_loss: 0.0368\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0399 - val_loss: 0.0361\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0393 - val_loss: 0.0358\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0389 - val_loss: 0.0358\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0387 - val_loss: 0.0356\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0385 - val_loss: 0.0356\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0384 - val_loss: 0.0355\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0383 - val_loss: 0.0357\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0382 - val_loss: 0.0352\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0380 - val_loss: 0.0353\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0379 - val_loss: 0.0351\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0378 - val_loss: 0.0352\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0377 - val_loss: 0.0349\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0376 - val_loss: 0.0350\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0376 - val_loss: 0.0351\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0376 - val_loss: 0.0350\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0374 - val_loss: 0.0348\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0373 - val_loss: 0.0348\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0373 - val_loss: 0.0348\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0372 - val_loss: 0.0349\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0372 - val_loss: 0.0348\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0372 - val_loss: 0.0347\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0371 - val_loss: 0.0347\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0371 - val_loss: 0.0347\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0371 - val_loss: 0.0348\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0370 - val_loss: 0.0346\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0370 - val_loss: 0.0345\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0369 - val_loss: 0.0345\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0369 - val_loss: 0.0345\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0369 - val_loss: 0.0343\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0368 - val_loss: 0.0344\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0368 - val_loss: 0.0343\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0368 - val_loss: 0.0342\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0367 - val_loss: 0.0342\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0367 - val_loss: 0.0343\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0367 - val_loss: 0.0342\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0342\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0367 - val_loss: 0.0341\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0341\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0366 - val_loss: 0.0342\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0341\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0341\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0341\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0366 - val_loss: 0.0341\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0340\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0365 - val_loss: 0.0340\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0341\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0364 - val_loss: 0.0339\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0339\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0341\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0364 - val_loss: 0.0339\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0338\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0338\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0338\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0361 - val_loss: 0.0338\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0361 - val_loss: 0.0339\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0338\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0361 - val_loss: 0.0338\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0361 - val_loss: 0.0338\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0361 - val_loss: 0.0338\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0361 - val_loss: 0.0338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 3/6 [01:40<01:45, 35.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 29ms/step - loss: 0.1329 - val_loss: 0.0500\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0460 - val_loss: 0.0385\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0418 - val_loss: 0.0378\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0408 - val_loss: 0.0370\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0401 - val_loss: 0.0363\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0394 - val_loss: 0.0359\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0390 - val_loss: 0.0358\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0388 - val_loss: 0.0356\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0386 - val_loss: 0.0356\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0385 - val_loss: 0.0355\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0384 - val_loss: 0.0355\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0383 - val_loss: 0.0353\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0382 - val_loss: 0.0354\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0381 - val_loss: 0.0353\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0379 - val_loss: 0.0355\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0379 - val_loss: 0.0351\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0377 - val_loss: 0.0350\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0377 - val_loss: 0.0351\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0377 - val_loss: 0.0349\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0375 - val_loss: 0.0350\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0374 - val_loss: 0.0349\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0374 - val_loss: 0.0349\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0374 - val_loss: 0.0350\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0373 - val_loss: 0.0348\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0373 - val_loss: 0.0348\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0373 - val_loss: 0.0348\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0372 - val_loss: 0.0347\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0372 - val_loss: 0.0348\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0372 - val_loss: 0.0347\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0371 - val_loss: 0.0346\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0371 - val_loss: 0.0346\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0371 - val_loss: 0.0346\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0370 - val_loss: 0.0345\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0370 - val_loss: 0.0346\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0370 - val_loss: 0.0344\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0369 - val_loss: 0.0344\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0369 - val_loss: 0.0344\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0369 - val_loss: 0.0344\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0369 - val_loss: 0.0343\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0368 - val_loss: 0.0344\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0368 - val_loss: 0.0343\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0368 - val_loss: 0.0343\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0368 - val_loss: 0.0343\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0367 - val_loss: 0.0343\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0367 - val_loss: 0.0343\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0367 - val_loss: 0.0342\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0367 - val_loss: 0.0342\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0367 - val_loss: 0.0343\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0367 - val_loss: 0.0343\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0367 - val_loss: 0.0342\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0342\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0342\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0366 - val_loss: 0.0342\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0341\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0366 - val_loss: 0.0342\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0366 - val_loss: 0.0341\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0340\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0340\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0365 - val_loss: 0.0341\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0365 - val_loss: 0.0340\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0341\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0341\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0339\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0364 - val_loss: 0.0340\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0339\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0340\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0340\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0363 - val_loss: 0.0340\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0364 - val_loss: 0.0341\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0340\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0340\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0363 - val_loss: 0.0339\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0362 - val_loss: 0.0340\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0362 - val_loss: 0.0339\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [02:08<01:04, 32.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 29ms/step - loss: 0.1300 - val_loss: 0.0498\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0521 - val_loss: 0.0445\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0478 - val_loss: 0.0427\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0467 - val_loss: 0.0420\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0461 - val_loss: 0.0418\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0457 - val_loss: 0.0418\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0455 - val_loss: 0.0417\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0452 - val_loss: 0.0417\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0451 - val_loss: 0.0414\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0449 - val_loss: 0.0413\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0448 - val_loss: 0.0413\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0446 - val_loss: 0.0414\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0445 - val_loss: 0.0411\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0444 - val_loss: 0.0413\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0444 - val_loss: 0.0413\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0443 - val_loss: 0.0412\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0442 - val_loss: 0.0413\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0442 - val_loss: 0.0411\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0441 - val_loss: 0.0411\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0441 - val_loss: 0.0409\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0441 - val_loss: 0.0409\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0440 - val_loss: 0.0410\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0440 - val_loss: 0.0409\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0439 - val_loss: 0.0411\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0439 - val_loss: 0.0408\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0439 - val_loss: 0.0408\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0439 - val_loss: 0.0408\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0438 - val_loss: 0.0408\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0438 - val_loss: 0.0409\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0437 - val_loss: 0.0407\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0437 - val_loss: 0.0406\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0437 - val_loss: 0.0406\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0436 - val_loss: 0.0406\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0436 - val_loss: 0.0405\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0436 - val_loss: 0.0405\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0435 - val_loss: 0.0404\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0435 - val_loss: 0.0405\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0435 - val_loss: 0.0404\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0434 - val_loss: 0.0403\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0434 - val_loss: 0.0403\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0434 - val_loss: 0.0403\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0433 - val_loss: 0.0402\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0433 - val_loss: 0.0402\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0433 - val_loss: 0.0402\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0432 - val_loss: 0.0401\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0432 - val_loss: 0.0401\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0432 - val_loss: 0.0401\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0432 - val_loss: 0.0400\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0431 - val_loss: 0.0401\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0431 - val_loss: 0.0400\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0431 - val_loss: 0.0400\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0430 - val_loss: 0.0400\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0430 - val_loss: 0.0399\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0430 - val_loss: 0.0399\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0430 - val_loss: 0.0398\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0429 - val_loss: 0.0398\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0429 - val_loss: 0.0398\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0429 - val_loss: 0.0397\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0429 - val_loss: 0.0398\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0398\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0397\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0397\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0398\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0397\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0397\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0397\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0428 - val_loss: 0.0397\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0398\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0426 - val_loss: 0.0398\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0396\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0426 - val_loss: 0.0397\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0427 - val_loss: 0.0399\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0425 - val_loss: 0.0398\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0425 - val_loss: 0.0396\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0426 - val_loss: 0.0396\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0400\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0425 - val_loss: 0.0396\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0425 - val_loss: 0.0396\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0425 - val_loss: 0.0396\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0425 - val_loss: 0.0396\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0425 - val_loss: 0.0396\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0425 - val_loss: 0.0396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 5/6 [02:50<00:36, 36.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 29ms/step - loss: 0.1299 - val_loss: 0.0497\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0520 - val_loss: 0.0444\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0477 - val_loss: 0.0427\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0466 - val_loss: 0.0420\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0461 - val_loss: 0.0418\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0457 - val_loss: 0.0419\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0454 - val_loss: 0.0418\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0452 - val_loss: 0.0420\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0451 - val_loss: 0.0415\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0449 - val_loss: 0.0413\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0448 - val_loss: 0.0413\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0447 - val_loss: 0.0415\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0445 - val_loss: 0.0412\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0444 - val_loss: 0.0412\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0444 - val_loss: 0.0411\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0443 - val_loss: 0.0412\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0442 - val_loss: 0.0413\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0442 - val_loss: 0.0411\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0441 - val_loss: 0.0411\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0441 - val_loss: 0.0410\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0440 - val_loss: 0.0409\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0440 - val_loss: 0.0410\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0439 - val_loss: 0.0409\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0439 - val_loss: 0.0410\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0439 - val_loss: 0.0407\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0438 - val_loss: 0.0407\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0438 - val_loss: 0.0407\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0437 - val_loss: 0.0408\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0437 - val_loss: 0.0407\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0437 - val_loss: 0.0406\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0437 - val_loss: 0.0406\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0436 - val_loss: 0.0406\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0436 - val_loss: 0.0406\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0436 - val_loss: 0.0405\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0435 - val_loss: 0.0406\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0435 - val_loss: 0.0404\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0435 - val_loss: 0.0404\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0435 - val_loss: 0.0404\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0434 - val_loss: 0.0404\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0434 - val_loss: 0.0403\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0434 - val_loss: 0.0404\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0434 - val_loss: 0.0403\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0433 - val_loss: 0.0403\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0433 - val_loss: 0.0402\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0433 - val_loss: 0.0401\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0432 - val_loss: 0.0401\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0432 - val_loss: 0.0402\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0432 - val_loss: 0.0402\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0431 - val_loss: 0.0404\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0431 - val_loss: 0.0402\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0431 - val_loss: 0.0401\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0431 - val_loss: 0.0401\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0430 - val_loss: 0.0401\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0430 - val_loss: 0.0400\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0430 - val_loss: 0.0400\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0430 - val_loss: 0.0399\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0430 - val_loss: 0.0400\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0429 - val_loss: 0.0400\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0429 - val_loss: 0.0399\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0429 - val_loss: 0.0400\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0429 - val_loss: 0.0399\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0429 - val_loss: 0.0398\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0429 - val_loss: 0.0400\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0429 - val_loss: 0.0401\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0428 - val_loss: 0.0400\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0428 - val_loss: 0.0400\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0399\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0428 - val_loss: 0.0398\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0428 - val_loss: 0.0400\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0428 - val_loss: 0.0398\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0428 - val_loss: 0.0397\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0427 - val_loss: 0.0398\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0427 - val_loss: 0.0402\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0399\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0398\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0400\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0427 - val_loss: 0.0400\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0427 - val_loss: 0.0400\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0427 - val_loss: 0.0398\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0427 - val_loss: 0.0400\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0399\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0426 - val_loss: 0.0398\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0426 - val_loss: 0.0399\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0427 - val_loss: 0.0397\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0427 - val_loss: 0.0400\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0426 - val_loss: 0.0398\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0398\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0426 - val_loss: 0.0397\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0426 - val_loss: 0.0400\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0399\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0426 - val_loss: 0.0397\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0398\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0426 - val_loss: 0.0403\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.0426 - val_loss: 0.0398\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0426 - val_loss: 0.0397\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0426 - val_loss: 0.0397\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0426 - val_loss: 0.0397\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0425 - val_loss: 0.0399\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0426 - val_loss: 0.0400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [03:22<00:00, 33.75s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries importing\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# additional modules\n",
        "import sys\n",
        "sys.path.append('../utils')\n",
        "import os\n",
        "# from evaluating import evaluating_change_point"
      ],
      "metadata": {
        "id": "89kFgRvXo-NJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_loss'], label='val', color='blue')\n",
        "plt.plot(history.history['loss'], label='train', color='red')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1NF74jeFlgG4",
        "outputId": "60f6170f-fae7-4f5a-93ca-997f44de0c25"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efbc6bd0410>]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I1qsmwoB55gl"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxz2bxjG3pf9",
        "outputId": "fb47e089-c8b3-4479-c58e-ebb5da8c35d1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.44913313, 0.49150798, 0.47379276, ..., 0.66506067,\n",
              "         0.519594  , 0.82725935],\n",
              "        [0.50649492, 0.4596195 , 0.29235379, ..., 0.63836422,\n",
              "         0.38864503, 0.82170867],\n",
              "        [0.51431891, 0.48552355, 0.4118939 , ..., 0.65100842,\n",
              "         0.48268317, 0.82070952],\n",
              "        ...,\n",
              "        [0.22396695, 0.38129389, 0.71372444, ..., 0.78121968,\n",
              "         0.9202579 , 0.82505076],\n",
              "        [0.28079812, 0.31316492, 0.32335163, ..., 0.76774622,\n",
              "         0.67485148, 0.82220571],\n",
              "        [0.28364874, 0.36428168, 0.62421699, ..., 0.77542955,\n",
              "         0.81018104, 0.82323413]],\n",
              "\n",
              "       [[0.50649492, 0.4596195 , 0.29235379, ..., 0.63836422,\n",
              "         0.38864503, 0.82170867],\n",
              "        [0.51431891, 0.48552355, 0.4118939 , ..., 0.65100842,\n",
              "         0.48268317, 0.82070952],\n",
              "        [0.49193137, 0.52060728, 0.39623627, ..., 0.66826495,\n",
              "         0.57171297, 0.8241006 ],\n",
              "        ...,\n",
              "        [0.28079812, 0.31316492, 0.32335163, ..., 0.76774622,\n",
              "         0.67485148, 0.82220571],\n",
              "        [0.28364874, 0.36428168, 0.62421699, ..., 0.77542955,\n",
              "         0.81018104, 0.82323413],\n",
              "        [0.4295221 , 0.3936817 , 0.62166591, ..., 0.27508002,\n",
              "         0.52874981, 0.58624879]],\n",
              "\n",
              "       [[0.51431891, 0.48552355, 0.4118939 , ..., 0.65100842,\n",
              "         0.48268317, 0.82070952],\n",
              "        [0.49193137, 0.52060728, 0.39623627, ..., 0.66826495,\n",
              "         0.57171297, 0.8241006 ],\n",
              "        [0.39277644, 0.57638021, 0.81201756, ..., 0.71553475,\n",
              "         0.89416262, 0.82653337],\n",
              "        ...,\n",
              "        [0.28364874, 0.36428168, 0.62421699, ..., 0.77542955,\n",
              "         0.81018104, 0.82323413],\n",
              "        [0.4295221 , 0.3936817 , 0.62166591, ..., 0.27508002,\n",
              "         0.52874981, 0.58624879],\n",
              "        [0.43158933, 0.4334768 , 0.42102352, ..., 0.2709691 ,\n",
              "         0.38366849, 0.6569892 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.4006307 , 0.45348153, 0.29535387, ..., 0.27404386,\n",
              "         0.52237456, 0.78625576],\n",
              "        [0.42334084, 0.44606419, 0.26544509, ..., 0.26775868,\n",
              "         0.47241025, 0.78848915],\n",
              "        [0.43870931, 0.44991829, 0.23536806, ..., 0.26974256,\n",
              "         0.43343083, 0.79108803],\n",
              "        ...,\n",
              "        [0.35340718, 0.36280779, 0.42957154, ..., 0.27766118,\n",
              "         0.57266235, 0.78379272],\n",
              "        [0.44421063, 0.42103443, 0.40421456, ..., 0.2528306 ,\n",
              "         0.62159401, 0.82548152],\n",
              "        [0.35444844, 0.30019354, 0.34852896, ..., 0.07836385,\n",
              "         0.64756316, 0.85616379]],\n",
              "\n",
              "       [[0.42334084, 0.44606419, 0.26544509, ..., 0.26775868,\n",
              "         0.47241025, 0.78848915],\n",
              "        [0.43870931, 0.44991829, 0.23536806, ..., 0.26974256,\n",
              "         0.43343083, 0.79108803],\n",
              "        [0.43098334, 0.4745083 , 0.30593476, ..., 0.2722998 ,\n",
              "         0.52931058, 0.79581611],\n",
              "        ...,\n",
              "        [0.44421063, 0.42103443, 0.40421456, ..., 0.2528306 ,\n",
              "         0.62159401, 0.82548152],\n",
              "        [0.35444844, 0.30019354, 0.34852896, ..., 0.07836385,\n",
              "         0.64756316, 0.85616379],\n",
              "        [0.38978807, 0.31463615, 0.20243235, ..., 0.0802506 ,\n",
              "         0.30298945, 0.84152072]],\n",
              "\n",
              "       [[0.43870931, 0.44991829, 0.23536806, ..., 0.26974256,\n",
              "         0.43343083, 0.79108803],\n",
              "        [0.43098334, 0.4745083 , 0.30593476, ..., 0.2722998 ,\n",
              "         0.52931058, 0.79581611],\n",
              "        [0.43949785, 0.46209791, 0.27938036, ..., 0.26020481,\n",
              "         0.47555007, 0.79208832],\n",
              "        ...,\n",
              "        [0.35444844, 0.30019354, 0.34852896, ..., 0.07836385,\n",
              "         0.64756316, 0.85616379],\n",
              "        [0.38978807, 0.31463615, 0.20243235, ..., 0.0802506 ,\n",
              "         0.30298945, 0.84152072],\n",
              "        [0.37804375, 0.37729266, 0.24159421, ..., 0.08461949,\n",
              "         0.52011125, 0.85173481]]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(anomaly[4], predicted_outlier[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "T_tCXSof3a6j",
        "outputId": "8b5618da-8d64-4af6-9248-9e8663abc133"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-9d2840fd0ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_outlier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (23304, 1) and (1,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# true outlier indices selection\n",
        "# true_outlier = [df.anomaly for df in result_csp]\n",
        "# import matplotlib as plt\n",
        "true_outlier = anomaly\n",
        "# pd.DataFrame(predicted_outlier[4]).plot(figsize=(12, 3), label='predictions', marker='o', markersize=5)\n",
        "# pd.DataFrame(true_outlier[4]).plot(marker='o', markersize=2)\n",
        "\n",
        "plt.plot(pd.DataFrame(true_outlier[4]), pd.DataFrame(true_outlier[4]))\n",
        "# plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "SKYEGXd2n68h",
        "outputId": "4bf9e535-0864-41a5-bcd4-358f23b59e81"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-9ad0829d9308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# pd.DataFrame(true_outlier[4]).plot(marker='o', markersize=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_outlier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_outlier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# plt.legend();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                     message='Support for multi-dimensional indexing')\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m                 \u001b[0;31m# we have definitely hit a pandas index or series object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;31m# cast to a numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (slice(None, None, None), None)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(true_outlier[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "u2zVZD8Buk7e",
        "outputId": "01684161-9721-4d6d-ba88-748801e27f30"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0      0.0\n",
              "1      0.0\n",
              "2      0.0\n",
              "3      0.0\n",
              "4      0.0\n",
              "...    ...\n",
              "25315  0.0\n",
              "25316  0.0\n",
              "25317  0.0\n",
              "25318  0.0\n",
              "25319  0.0\n",
              "\n",
              "[25320 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1037793d-7ec2-4b63-9419-385fd77ce69d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25315</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25316</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25317</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25318</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25319</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25320 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1037793d-7ec2-4b63-9419-385fd77ce69d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1037793d-7ec2-4b63-9419-385fd77ce69d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1037793d-7ec2-4b63-9419-385fd77ce69d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_outlier[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtPvA2wsn_5J",
        "outputId": "d44fb81c-4376-4c25-f776-335210c4ccda"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       ...,\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# binary classification metrics calculation\n",
        "binary = evaluating_change_point(pd.DataFrame(true_outlier[0]), pd.DataFrame(predicted_outlier[0]), metric='binary', numenta_time='60 sec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPOqi4ysoloV",
        "outputId": "7d965c78-6ae0-4cab-e92f-e91ef2d80442"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False Alarm Rate 0    0.0\n",
            "dtype: float64 %\n",
            "Missing Alarm Rate 0   NaN\n",
            "dtype: float64 %\n",
            "F1 metric 0   NaN\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluating_change_point(true, prediction, metric='nab', numenta_time=None):\n",
        "    \"\"\"\n",
        "    true - both:\n",
        "                list of pandas Series with binary int labels\n",
        "                pandas Series with binary int labels\n",
        "    prediction - both:\n",
        "                      list of pandas Series with binary int labels\n",
        "                      pandas Series with binary int labels\n",
        "    metric: 'nab', 'binary' (FAR, MAR), 'average_delay'\n",
        "                \n",
        "    \"\"\"\n",
        "    \n",
        "    def binary(true, prediction):      \n",
        "        \"\"\"\n",
        "        true - true binary series with 1 as anomalies\n",
        "        prediction - trupredicted binary series with 1 as anomalies\n",
        "        \"\"\"\n",
        "        def single_binary(true,prediction):\n",
        "            true_ = true == 1 \n",
        "            prediction_ = prediction == 1\n",
        "            TP = (true_ & prediction_).sum()\n",
        "            TN = (~true_ & ~prediction_).sum()\n",
        "            FP = (~true_ & prediction_).sum()\n",
        "            FN = (true_ & ~prediction_).sum()\n",
        "            return TP,TN,FP,FN\n",
        "            \n",
        "        if type(true) != type(list()):\n",
        "            TP,TN,FP,FN = single_binary(true,prediction)\n",
        "        else:\n",
        "            TP,TN,FP,FN = 0,0,0,0\n",
        "            for i in range(len(true)):\n",
        "                TP_,TN_,FP_,FN_ = single_binary(true[i],prediction[i])\n",
        "                TP,TN,FP,FN = TP+TP_,TN+TN_,FP+FP_,FN+FN_       \n",
        "    \n",
        "        f1 = round(TP/(TP+(FN+FP)/2), 2)\n",
        "        print(f'False Alarm Rate {round(FP/(FP+TN)*100,2)} %' )\n",
        "        print(f'Missing Alarm Rate {round(FN/(FN+TP)*100,2)} %')\n",
        "        print(f'F1 metric {f1}')\n",
        "        return f1\n",
        "    \n",
        "    def average_delay(detecting_boundaries, prediction):\n",
        "        \n",
        "        def single_average_delay(detecting_boundaries, prediction):\n",
        "            missing = 0\n",
        "            detectHistory = []\n",
        "            for couple in detecting_boundaries:\n",
        "                t1 = couple[0]\n",
        "                t2 = couple[1]\n",
        "                if prediction[t1:t2].sum()==0:\n",
        "                    missing+=1\n",
        "                else:\n",
        "                    detectHistory.append(prediction[prediction ==1][t1:t2].index[0]-t1)\n",
        "            return missing, detectHistory\n",
        "            \n",
        "        \n",
        "        if type(prediction) != type(list()):\n",
        "            missing, detectHistory = single_average_delay(detecting_boundaries, prediction)\n",
        "        else:\n",
        "            missing, detectHistory = 0, []\n",
        "            for i in range(len(prediction)):\n",
        "                missing_, detectHistory_ = single_average_delay(detecting_boundaries[i], prediction[i])\n",
        "                missing, detectHistory = missing+missing_, detectHistory+detectHistory_\n",
        "\n",
        "        add = pd.Series(detectHistory).mean()\n",
        "        print('Average delay', add)\n",
        "        print(f'A number of missed CPs = {missing}')\n",
        "        return add\n",
        "    \n",
        "    def evaluate_nab(detecting_boundaries, prediction, table_of_coef=None):\n",
        "        \"\"\"\n",
        "        Scoring labeled time series by means of\n",
        "        Numenta Anomaly Benchmark methodics\n",
        "        Parameters\n",
        "        ----------\n",
        "        detecting_boundaries: list of list of two float values\n",
        "            The list of lists of left and right boundary indices\n",
        "            for scoring results of labeling\n",
        "        prediction: pd.Series with timestamp indices, in which 1 \n",
        "            is change point, and 0 in other case. \n",
        "        table_of_coef: pandas array (3x4) of float values\n",
        "            Table of coefficients for NAB score function\n",
        "            indeces: 'Standart','LowFP','LowFN'\n",
        "            columns:'A_tp','A_fp','A_tn','A_fn'\n",
        "        Returns\n",
        "        -------\n",
        "        Scores: numpy array, shape of 3, float\n",
        "            Score for 'Standart','LowFP','LowFN' profile \n",
        "        Scores_null: numpy array, shape 3, float\n",
        "            Null score for 'Standart','LowFP','LowFN' profile             \n",
        "        Scores_perfect: numpy array, shape 3, float\n",
        "            Perfect Score for 'Standart','LowFP','LowFN' profile  \n",
        "        \"\"\"\n",
        "        def single_evaluate_nab(detecting_boundaries, prediction, table_of_coef=None, name_of_dataset=None):\n",
        "            if table_of_coef is None:\n",
        "                table_of_coef = pd.DataFrame([[1.0,-0.11,1.0,-1.0],\n",
        "                                     [1.0,-0.22,1.0,-1.0],\n",
        "                                      [1.0,-0.11,1.0,-2.0]])\n",
        "                table_of_coef.index = ['Standart','LowFP','LowFN']\n",
        "                table_of_coef.index.name = \"Metric\"\n",
        "                table_of_coef.columns = ['A_tp','A_fp','A_tn','A_fn']\n",
        "\n",
        "            alist = detecting_boundaries.copy()\n",
        "            prediction = prediction.copy()\n",
        "\n",
        "            Scores, Scores_perfect, Scores_null=[], [], []\n",
        "            for profile in ['Standart', 'LowFP', 'LowFN']:       \n",
        "                A_tp = table_of_coef['A_tp'][profile]\n",
        "                A_fp = table_of_coef['A_fp'][profile]\n",
        "                A_fn = table_of_coef['A_fn'][profile]\n",
        "                def sigm_scale(y, A_tp, A_fp, window=1):\n",
        "                    return (A_tp-A_fp)*(1/(1+np.exp(5*y/window))) + A_fp\n",
        "\n",
        "                #First part\n",
        "                score = 0\n",
        "                if len(alist)>0:\n",
        "                    score += prediction[:alist[0][0]].sum()*A_fp\n",
        "                else:\n",
        "                    score += prediction.sum()*A_fp\n",
        "                #second part\n",
        "                for i in range(len(alist)):\n",
        "                    if i<=len(alist)-2:\n",
        "                        win_space = prediction[alist[i][0]:alist[i+1][0]].copy()\n",
        "                    else:\n",
        "                        win_space = prediction[alist[i][0]:].copy()\n",
        "                    win_fault = prediction[alist[i][0]:alist[i][1]]\n",
        "                    slow_width = int(len(win_fault)/4)\n",
        "\n",
        "                    if len(win_fault) + slow_width >= len(win_space):\n",
        "                        print(f'Intersection of the windows of too wide widths for dataset {name_of_dataset}')\n",
        "                        win_fault_slow = win_fault.copy()\n",
        "                    else:\n",
        "                        win_fault_slow= win_space[:len(win_fault)  +  slow_width]\n",
        "\n",
        "                    win_fp = win_space[-len(win_fault_slow):]\n",
        "\n",
        "                    if win_fault_slow.sum() == 0:\n",
        "                        score+=A_fn\n",
        "                    else:\n",
        "                        #to get the first index\n",
        "                        tr = pd.Series(win_fault_slow.values,index = range(-len(win_fault), len(win_fault_slow)-len(win_fault)))\n",
        "                        tr_values= tr[tr==1].index[0]\n",
        "                        tr_score = sigm_scale(tr_values, A_tp,A_fp,slow_width)\n",
        "                        score += tr_score\n",
        "                        score += win_fp.sum()*A_fp\n",
        "                Scores.append(score)\n",
        "                Scores_perfect.append(len(alist)*A_tp)\n",
        "                Scores_null.append(len(alist)*A_fn)\n",
        "            return np.array([np.array(Scores),np.array(Scores_null), np.array(Scores_perfect)])\n",
        "       #======      \n",
        "        if type(prediction) != type(list()):\n",
        "            matrix = single_evaluate_nab(detecting_boundaries, prediction, table_of_coef=table_of_coef)\n",
        "        else:\n",
        "            matrix = np.zeros((3,3))\n",
        "            for i in range(len(prediction)):\n",
        "                matrix_ = single_evaluate_nab(detecting_boundaries[i], prediction[i], table_of_coef=table_of_coef,name_of_dataset=i)\n",
        "                matrix = matrix + matrix_      \n",
        "                \n",
        "        results = {}\n",
        "        desc = ['Standart', 'LowFP', 'LowFN'] \n",
        "        for t, profile_name in enumerate(desc):\n",
        "            results[profile_name] = round(100*(matrix[0,t]-matrix[1,t])/(matrix[2,t]-matrix[1,t]), 2)\n",
        "            print(profile_name,' - ', results[profile_name])\n",
        "        \n",
        "        return results\n",
        "            \n",
        "            \n",
        "    #=========================================================================\n",
        "    if type(true) != type(list()):\n",
        "        true_items = true[true==1].index\n",
        "    else:\n",
        "        true_items = [true[i][true[i]==1].index for i in range(len(true))]\n",
        "        \n",
        "\n",
        "    if not metric=='binary':\n",
        "        def single_detecting_boundaries(true, numenta_time, true_items):\n",
        "            detecting_boundaries=[]\n",
        "            td = pd.Timedelta(numenta_time) if numenta_time is not None else pd.Timedelta((true.index[-1]-true.index[0])/len(true_items))  \n",
        "            for val in true_items:\n",
        "                detecting_boundaries.append([val, val + td])\n",
        "            return detecting_boundaries\n",
        "        \n",
        "        if type(true) != type(list()):\n",
        "            detecting_boundaries = single_detecting_boundaries(true=true, numenta_time=numenta_time, true_items=true_items)\n",
        "        else:\n",
        "            detecting_boundaries=[]\n",
        "            for i in range(len(true)):\n",
        "                detecting_boundaries.append(single_detecting_boundaries(true=true[i], numenta_time=numenta_time, true_items=true_items[i]))\n",
        "\n",
        "    if metric== 'nab':\n",
        "        return evaluate_nab(detecting_boundaries, prediction)\n",
        "    elif metric=='average_delay':\n",
        "        return average_delay(detecting_boundaries, prediction)\n",
        "    elif metric== 'binary':\n",
        "        return binary(true, prediction)"
      ],
      "metadata": {
        "id": "gJ82gCFJrdGv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1pugB0TQrj-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}